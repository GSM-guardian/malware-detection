import os
import time
import argparse

from model import *
from utils import *
from dataset import *

import torch
import torch.nn as nn
import torch.optim as optim

from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms

def main():
    parser = argparse.ArgumentParser(description='Malware Detection parameters',
                                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument('--lr', default=0.01, type=float, dest='lr')
    parser.add_argument('--batch_size', default=256, type=int, dest='batch_size')
    parser.add_argument('--num_epoch', default=10, type=int, dest='num_epoch')

    parser.add_argument('--task', default='sequences', choices=['sequences', 'images'], type=str, dest='task')

    parser.add_argument('--data_dir', default='./datasets/train', type=str, dest='data_dir')
    parser.add_argument('--ckpt_dir', default='./checkpoint', type=str, dest='ckpt_dir')
    parser.add_argument('--log_dir', default='./log', type=str, dest='log_dir')
    
    parser.add_argument('--mode', default='train', choices=['train', 'test'], type=str, dest='mode')
    parser.add_argument('--train_continue', default='off', type=str, dest='train_continue')

    args = parser.parse_args()

    lr = args.lr
    batch_size = args.batch_size
    num_epoch = args.num_epoch

    task = args.task

    data_dir = args.data_dir
    ckpt_dir = args.ckpt_dir
    log_dir = args.log_dir

    mode = args.mode
    train_continue = args.train_continue

    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

    # data setting
    train_dataset = Dataset(data_dir=data_dir, task=task, transfrom=None)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)
    num_data_train = len(train_dataset)
    num_batch_train = np.ceil(num_data_train / batch_size)

    # network load
    net = MalConv(input_length=2000000, kernel_size=500).to(device)

    # loss function
    fn_loss = nn.BCEWithLogitsLoss()

    # optimizer
    optim = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.5, 0.9))

    st_epoch = 0

    if mode == 'train':
        if train_dataset == 'on':
            net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)

        for epoch in range(st_epoch + 1, num_epoch + 1):
            net.train()
            loss_arr = []

            for batch, data in enumerate(train_loader, 1):
                start = time.time()

                input = data['input'].to(device)
                label = data['label'].to(device)

                output = net(input)

                optim.zero_grad()

                loss = fn_loss(output, label)
                loss.backward()

                optim.step()

                # loss function
                loss_arr += [loss.item()]

                step_time = time.time() - start

                print('TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | TIME %.4f | LOSS %.4f' %
                      (epoch, num_epoch, batch, num_batch_train, np.mean(step_time), np.mean(loss_arr)))

                if epoch % 2 == 0:
                    save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch)